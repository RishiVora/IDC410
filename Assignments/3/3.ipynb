{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"> Assignment 3 </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "def data(file_path):\n",
    "    f = np.loadtxt(file_path, delimiter=',', skiprows=1)\n",
    "    y, X = np.hsplit(f, [1])\n",
    "    return X/255, np.eye(10)[y.astype(int).flatten()]\n",
    "\n",
    "X, y = data('mnist_train.csv')\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Weights:  [[0.47951206 0.07883346 0.48735643 ... 0.692196   0.47569832 0.08443742]\n",
      " [0.8893862  0.22574533 0.81552453 ... 0.16170143 0.22919909 0.54176543]\n",
      " [0.80632959 0.97134511 0.77494957 ... 0.46563653 0.03842637 0.90185028]\n",
      " ...\n",
      " [0.88002032 0.59986718 0.30881364 ... 0.83595749 0.57593267 0.0757345 ]\n",
      " [0.66841381 0.51205339 0.20764443 ... 0.61761481 0.75601462 0.03723043]\n",
      " [0.90099395 0.64954074 0.8184798  ... 0.30199948 0.41245826 0.99797078]]\n",
      "\n",
      "Bias:  [[0.64011487 0.79749696 0.16366571 0.1148001  0.88020093 0.34260314\n",
      "  0.12497425 0.82650062 0.06666921 0.07837925]]\n",
      "\n",
      "Loss:  5.5277321729544635\n"
     ]
    }
   ],
   "source": [
    "class Network:\n",
    "    def __init__(self, X, y, W, b, learning_rate):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def softmax(self, z):\n",
    "        e_z = np.exp(z)\n",
    "        return e_z / e_z.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    def cross_entropy_loss(self, y_true, y_pred):\n",
    "        return -np.sum(y_true * np.log(y_pred + 1e-100)) / samples\n",
    "    \n",
    "    def feed_forward(self):\n",
    "        z = np.dot(self.X, self.W) + self.b\n",
    "        a = self.softmax(z)\n",
    "        return a\n",
    "    \n",
    "    def backward_propagation(self):\n",
    "        dz = self.y_pred - self.y\n",
    "        dW = (1/samples) * np.dot(self.X.T, dz)\n",
    "        db = (1/samples) * np.sum(dz, axis=0, keepdims=True)  \n",
    "        self.W = self.W - self.learning_rate * dW\n",
    "        self.b = self.b - self.learning_rate * db\n",
    "        return self.W, self.b\n",
    "    \n",
    "    def train(self):\n",
    "        self.y_pred = self.feed_forward()\n",
    "        self.loss = self.cross_entropy_loss(self.y, self.y_pred)\n",
    "        self.W, self.b = self.backward_propagation()\n",
    "        return self.W, self.b, self.loss\n",
    "    \n",
    "\n",
    "classes = 10\n",
    "samples = 60000\n",
    "image_dim = 28\n",
    "W = np.random.rand(image_dim**2, classes)\n",
    "b = np.random.rand(1, classes)\n",
    "learning_rate = 0.01\n",
    "\n",
    "nn = Network(X, y, W, b, learning_rate)\n",
    "W, b, loss = nn.train()\n",
    "print(\"\\nWeights: \", W)\n",
    "print(\"\\nBias: \", b)\n",
    "print(\"\\nLoss: \", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Weights:  [array([[0.10284916, 0.34005148, 0.44305252, ..., 0.37313268, 0.74045841,\n",
      "        0.21921329],\n",
      "       [0.14333583, 0.47552898, 0.9919205 , ..., 0.69433463, 0.28326331,\n",
      "        0.05895841],\n",
      "       [0.62691706, 0.27681509, 0.44158828, ..., 0.72428842, 0.31629683,\n",
      "        0.39631339],\n",
      "       ...,\n",
      "       [0.23593546, 0.98690998, 0.55848149, ..., 0.98255528, 0.46225439,\n",
      "        0.80411653],\n",
      "       [0.53341282, 0.95722102, 0.38196526, ..., 0.12261942, 0.19300923,\n",
      "        0.48934213],\n",
      "       [0.5472537 , 0.43592915, 0.88781541, ..., 0.3662114 , 0.58175571,\n",
      "        0.31459205]]), array([[8.60362137e-01, 1.04343652e-01, 1.60345292e-01, 7.21381962e-01,\n",
      "        5.54623260e-01, 9.72781458e-01, 7.38364326e-01, 5.28075515e-01,\n",
      "        5.23957456e-01, 3.41672558e-01],\n",
      "       [3.15814865e-02, 6.07937192e-01, 6.53679262e-01, 3.27453818e-01,\n",
      "        6.76777394e-01, 4.62488664e-01, 9.01266500e-01, 6.56402229e-01,\n",
      "        1.15478851e-01, 3.94373869e-01],\n",
      "       [1.11830652e-01, 4.88954303e-01, 4.40703966e-01, 5.42151756e-01,\n",
      "        4.06791740e-01, 5.96882133e-02, 9.74115401e-02, 4.47540569e-02,\n",
      "        2.60298192e-01, 3.98298410e-01],\n",
      "       [2.43115898e-01, 2.22040820e-01, 9.00049287e-01, 3.63986226e-01,\n",
      "        4.92751773e-02, 5.90859878e-01, 6.54872154e-05, 7.49810917e-01,\n",
      "        1.32296892e-01, 1.12036079e-01],\n",
      "       [3.62119521e-01, 1.10666591e-01, 3.02133539e-01, 8.85039803e-01,\n",
      "        2.31804594e-01, 3.00205993e-01, 5.84297768e-01, 8.15377580e-01,\n",
      "        7.69070961e-01, 1.88108740e-01],\n",
      "       [2.47954015e-01, 4.00014202e-01, 1.82875693e-01, 1.06916816e-01,\n",
      "        8.43487772e-01, 4.10341911e-02, 6.72296829e-01, 6.35543788e-02,\n",
      "        5.59730263e-01, 1.21540565e-01],\n",
      "       [1.81694661e-01, 7.24313084e-01, 9.66243880e-01, 3.94107216e-01,\n",
      "        6.69694175e-01, 4.43933635e-01, 5.61014032e-01, 2.15449678e-01,\n",
      "        3.16888505e-01, 9.43661710e-01],\n",
      "       [7.26897362e-01, 3.37770379e-01, 5.69659940e-01, 6.86420602e-01,\n",
      "        4.14001694e-01, 9.51768669e-01, 9.10145095e-01, 7.46115166e-02,\n",
      "        5.68831973e-01, 2.30553105e-01],\n",
      "       [8.62730591e-01, 1.86902659e-01, 8.22006140e-01, 5.80240654e-01,\n",
      "        1.72571519e-01, 5.49427226e-01, 2.87475603e-01, 5.35622074e-01,\n",
      "        9.41543437e-01, 4.73551497e-01],\n",
      "       [3.36881379e-01, 1.51356104e-03, 7.85946307e-02, 2.58761157e-01,\n",
      "        2.36028717e-01, 4.22749362e-01, 4.65400623e-01, 1.46906071e-01,\n",
      "        3.54040360e-01, 6.87750932e-01],\n",
      "       [9.11124089e-01, 9.35535702e-01, 6.50903693e-01, 3.60959565e-01,\n",
      "        5.02542209e-01, 3.23817787e-01, 4.26370994e-01, 2.61088012e-01,\n",
      "        5.16706073e-01, 7.64992661e-01],\n",
      "       [1.78364256e-01, 5.65618246e-01, 6.35714169e-01, 6.77891786e-01,\n",
      "        8.03620744e-01, 3.52964900e-01, 3.29575595e-01, 9.02938126e-01,\n",
      "        6.91235630e-01, 3.25330476e-01],\n",
      "       [3.67138788e-01, 7.43556135e-01, 5.48680418e-01, 2.17252335e-01,\n",
      "        8.17147207e-01, 3.87637970e-01, 3.71010347e-01, 8.33626689e-01,\n",
      "        3.11320890e-01, 7.28070476e-01],\n",
      "       [2.73022645e-01, 1.77025933e-01, 2.29042537e-01, 8.40166642e-02,\n",
      "        7.15149803e-01, 2.54087329e-01, 8.74839841e-01, 5.27097859e-01,\n",
      "        2.07610093e-01, 3.07890699e-01],\n",
      "       [4.24628125e-01, 3.92948398e-01, 1.66715279e-01, 7.95387978e-01,\n",
      "        5.56544761e-01, 2.40916470e-01, 8.67509577e-02, 5.34065349e-01,\n",
      "        5.17429304e-01, 4.96080178e-02]])]\n",
      "\n",
      "Bias:  [array([[0.8131805 , 0.32090315, 0.09229554, 0.09523869, 0.78505156,\n",
      "        0.8327844 , 0.87539375, 0.29382316, 0.6475573 , 0.53225385,\n",
      "        0.87531318, 0.35518635, 0.61661557, 0.90694634, 0.66774526]]), array([[0.3480256 , 0.96584938, 0.94956374, 0.15607082, 0.62568527,\n",
      "        0.89873564, 0.21454596, 0.37487853, 0.48633038, 0.60468442]])]\n",
      "\n",
      "Loss:  2.482737627871978\n"
     ]
    }
   ],
   "source": [
    "class Network:\n",
    "    def __init__(self, X, y, W, b, learning_rate):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def softmax(self, z):\n",
    "        e_z = np.exp(z)\n",
    "        return e_z / e_z.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    def cross_entropy_loss(self, y_true, y_pred):\n",
    "        return -np.sum(y_true * np.log(y_pred + 1e-100)) / samples\n",
    "    \n",
    "    def feed_forward(self, X, W, b):\n",
    "        # print((np.dot(X, W[0]).shape, b[0].shape))\n",
    "        a1 = self.sigmoid((np.dot(X, W[0]) + b[0]).astype(float))\n",
    "        a2 = self.softmax((np.dot(a1, W[1]) + b[1]).astype(float))\n",
    "        a = [a1, a2]\n",
    "        return a\n",
    "    \n",
    "    def backward_propagation(self, X, a, W, b):\n",
    "        dz2 = a[1] - self.y\n",
    "        dW2 = (1/samples) * np.dot(a[0].T, dz2)\n",
    "        db2 = (1/samples) * np.sum(dz2, axis=0, keepdims=True)\n",
    "        dz1 = np.dot(dz2, W[1].T) * (a[0] * (1 - a[0])) \n",
    "        dW1 = (1/samples) * np.dot(X.T, dz1)\n",
    "        db1 = (1/samples) * np.sum(dz1, axis=0, keepdims=True)\n",
    "        W[0] = W[0] - self.learning_rate * dW1\n",
    "        W[1] = W[1] - self.learning_rate * dW2\n",
    "        b[0] = b[0] - self.learning_rate * db1\n",
    "        b[1] = b[1] - self.learning_rate * db2\n",
    "        return W, b\n",
    "    \n",
    "    def train(self):\n",
    "        a = self.feed_forward(self.X, self.W, self.b)\n",
    "        loss = self.cross_entropy_loss(self.y, a[1])\n",
    "        W, b = self.backward_propagation(self.X, a, self.W, self.b)\n",
    "        return W, b, loss\n",
    "\n",
    "\n",
    "classes = 10\n",
    "samples = 60000\n",
    "image_dim = 28\n",
    "n = 15\n",
    "W = [np.random.rand(image_dim**2, n), np.random.rand(n, classes)]\n",
    "b = [np.random.rand(1, n), np.random.rand(1, classes)]\n",
    "learning_rate = 0.01\n",
    "\n",
    "nn = Network(X, y, W, b, learning_rate)\n",
    "W, b, loss = nn.train()\n",
    "print(\"\\nWeights: \", W)\n",
    "print(\"\\nBias: \", b)\n",
    "print(\"\\nLoss: \", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Weights:  [array([[0.10284916, 0.34005148, 0.44305252, ..., 0.37313268, 0.74045841,\n",
      "        0.21921329],\n",
      "       [0.14333583, 0.47552898, 0.9919205 , ..., 0.69433463, 0.28326331,\n",
      "        0.05895841],\n",
      "       [0.62691706, 0.27681509, 0.44158828, ..., 0.72428842, 0.31629683,\n",
      "        0.39631339],\n",
      "       ...,\n",
      "       [0.23593546, 0.98690998, 0.55848149, ..., 0.98255528, 0.46225439,\n",
      "        0.80411653],\n",
      "       [0.53341282, 0.95722102, 0.38196526, ..., 0.12261942, 0.19300923,\n",
      "        0.48934213],\n",
      "       [0.5472537 , 0.43592915, 0.88781541, ..., 0.3662114 , 0.58175571,\n",
      "        0.31459205]]), array([[8.60362137e-01, 1.04343652e-01, 1.60345292e-01, 7.21381962e-01,\n",
      "        5.54623260e-01, 9.72781458e-01, 7.38364326e-01, 5.28075515e-01,\n",
      "        5.23957456e-01, 3.41672558e-01],\n",
      "       [3.15814865e-02, 6.07937192e-01, 6.53679262e-01, 3.27453818e-01,\n",
      "        6.76777394e-01, 4.62488664e-01, 9.01266500e-01, 6.56402229e-01,\n",
      "        1.15478851e-01, 3.94373869e-01],\n",
      "       [1.11830652e-01, 4.88954303e-01, 4.40703966e-01, 5.42151756e-01,\n",
      "        4.06791740e-01, 5.96882133e-02, 9.74115401e-02, 4.47540569e-02,\n",
      "        2.60298192e-01, 3.98298410e-01],\n",
      "       [2.43115898e-01, 2.22040820e-01, 9.00049287e-01, 3.63986226e-01,\n",
      "        4.92751773e-02, 5.90859878e-01, 6.54872154e-05, 7.49810917e-01,\n",
      "        1.32296892e-01, 1.12036079e-01],\n",
      "       [3.62119521e-01, 1.10666591e-01, 3.02133539e-01, 8.85039803e-01,\n",
      "        2.31804594e-01, 3.00205993e-01, 5.84297768e-01, 8.15377580e-01,\n",
      "        7.69070961e-01, 1.88108740e-01],\n",
      "       [2.47954015e-01, 4.00014202e-01, 1.82875693e-01, 1.06916816e-01,\n",
      "        8.43487772e-01, 4.10341911e-02, 6.72296829e-01, 6.35543788e-02,\n",
      "        5.59730263e-01, 1.21540565e-01],\n",
      "       [1.81694661e-01, 7.24313084e-01, 9.66243880e-01, 3.94107216e-01,\n",
      "        6.69694175e-01, 4.43933635e-01, 5.61014032e-01, 2.15449678e-01,\n",
      "        3.16888505e-01, 9.43661710e-01],\n",
      "       [7.26897362e-01, 3.37770379e-01, 5.69659940e-01, 6.86420602e-01,\n",
      "        4.14001694e-01, 9.51768669e-01, 9.10145095e-01, 7.46115166e-02,\n",
      "        5.68831973e-01, 2.30553105e-01],\n",
      "       [8.62730591e-01, 1.86902659e-01, 8.22006140e-01, 5.80240654e-01,\n",
      "        1.72571519e-01, 5.49427226e-01, 2.87475603e-01, 5.35622074e-01,\n",
      "        9.41543437e-01, 4.73551497e-01],\n",
      "       [3.36881379e-01, 1.51356104e-03, 7.85946307e-02, 2.58761157e-01,\n",
      "        2.36028717e-01, 4.22749362e-01, 4.65400623e-01, 1.46906071e-01,\n",
      "        3.54040360e-01, 6.87750932e-01],\n",
      "       [9.11124089e-01, 9.35535702e-01, 6.50903693e-01, 3.60959565e-01,\n",
      "        5.02542209e-01, 3.23817787e-01, 4.26370994e-01, 2.61088012e-01,\n",
      "        5.16706073e-01, 7.64992661e-01],\n",
      "       [1.78364256e-01, 5.65618246e-01, 6.35714169e-01, 6.77891786e-01,\n",
      "        8.03620744e-01, 3.52964900e-01, 3.29575595e-01, 9.02938126e-01,\n",
      "        6.91235630e-01, 3.25330476e-01],\n",
      "       [3.67138788e-01, 7.43556135e-01, 5.48680418e-01, 2.17252335e-01,\n",
      "        8.17147207e-01, 3.87637970e-01, 3.71010347e-01, 8.33626689e-01,\n",
      "        3.11320890e-01, 7.28070476e-01],\n",
      "       [2.73022645e-01, 1.77025933e-01, 2.29042537e-01, 8.40166642e-02,\n",
      "        7.15149803e-01, 2.54087329e-01, 8.74839841e-01, 5.27097859e-01,\n",
      "        2.07610093e-01, 3.07890699e-01],\n",
      "       [4.24628125e-01, 3.92948398e-01, 1.66715279e-01, 7.95387978e-01,\n",
      "        5.56544761e-01, 2.40916470e-01, 8.67509577e-02, 5.34065349e-01,\n",
      "        5.17429304e-01, 4.96080178e-02]])]\n",
      "\n",
      "Bias:  [array([[0.8131805 , 0.32090315, 0.09229554, 0.09523869, 0.78505156,\n",
      "        0.8327844 , 0.87539375, 0.29382316, 0.6475573 , 0.53225385,\n",
      "        0.87531318, 0.35518635, 0.61661557, 0.90694634, 0.66774526]]), array([[0.3480256 , 0.96584938, 0.94956374, 0.15607082, 0.62568527,\n",
      "        0.89873564, 0.21454596, 0.37487853, 0.48633038, 0.60468442]])]\n"
     ]
    }
   ],
   "source": [
    "class Network():\n",
    "    def __init__(self, X, y, weights, biases, learning_rate):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.weights = weights\n",
    "        self.biases = biases\n",
    "        self.n_layers = len(weights)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def sigmoid_derivative(self, z):\n",
    "        return self.sigmoid(z) * (1 - self.sigmoid(z))\n",
    "\n",
    "    def softmax(self, z):\n",
    "        e_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return e_z / np.sum(e_z, axis=1, keepdims=True)\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_true):\n",
    "        return -np.sum(y_true * np.log(y_pred), axis=1)\n",
    "\n",
    "    def cross_entropy_derivative(self, y_pred, y_true):\n",
    "        return y_pred - y_true\n",
    "\n",
    "    def backpropagation(self):\n",
    "        z = [None] * self.n_layers\n",
    "        a = [None] * self.n_layers\n",
    "        a.insert(0, self.X)\n",
    "        for i in range(self.n_layers - 1):\n",
    "            z[i] = np.dot(a[i], self.weights[i]) + self.biases[i]\n",
    "            a[i + 1] = self.sigmoid(z[i])\n",
    "        z[-1] = np.dot(a[-2], self.weights[-1]) + self.biases[-1]\n",
    "        a[-1] = self.softmax(z[-1])\n",
    "\n",
    "        # loss = np.mean(self.cross_entropy(a[-1], self.y)) #enable this if you want to see the loss\n",
    "\n",
    "        # Backward pass\n",
    "        deltas = [None] * self.n_layers\n",
    "        deltas[-1] = self.cross_entropy_derivative(a[-1], self.y)\n",
    "        for i in reversed(range(self.n_layers - 1)):\n",
    "            deltas[i] = np.dot(deltas[i + 1], self.weights[i + 1].T) * self.sigmoid_derivative(z[i])\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            self.weights[i] -= self.learning_rate * np.dot(a[i].T, deltas[i]) / samples\n",
    "            self.biases[i] -= self.learning_rate * np.sum(deltas[i], axis=0, keepdims=True) / samples\n",
    "\n",
    "        return weights, biases\n",
    "\n",
    "\n",
    "classes = 10 # number of neurons in output layer\n",
    "samples = 60000 # number of samples\n",
    "n_input = 784 # image dimension\n",
    "layers = [n_input, 20, 15, classes]  # Input layer, two hidden layers, output layer\n",
    "weights = [np.random.randn(layers[i], layers[i + 1]) for i in range(len(layers) - 1)]\n",
    "biases = [np.zeros((1, layers[i + 1])) for i in range(len(layers) - 1)]\n",
    "learning_rate = 0.01\n",
    "\n",
    "X, y = data('mnist_train.csv')\n",
    "nn = Network(X, y, weights, biases, learning_rate)\n",
    "weights, biases = nn.backpropagation()\n",
    "print(\"\\nWeights: \", W)\n",
    "print(\"\\nBias: \", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Weights:  [array([[0.10284916, 0.34005148, 0.44305252, ..., 0.37313268, 0.74045841,\n",
      "        0.21921329],\n",
      "       [0.14333583, 0.47552898, 0.9919205 , ..., 0.69433463, 0.28326331,\n",
      "        0.05895841],\n",
      "       [0.62691706, 0.27681509, 0.44158828, ..., 0.72428842, 0.31629683,\n",
      "        0.39631339],\n",
      "       ...,\n",
      "       [0.23593546, 0.98690998, 0.55848149, ..., 0.98255528, 0.46225439,\n",
      "        0.80411653],\n",
      "       [0.53341282, 0.95722102, 0.38196526, ..., 0.12261942, 0.19300923,\n",
      "        0.48934213],\n",
      "       [0.5472537 , 0.43592915, 0.88781541, ..., 0.3662114 , 0.58175571,\n",
      "        0.31459205]]), array([[8.60362137e-01, 1.04343652e-01, 1.60345292e-01, 7.21381962e-01,\n",
      "        5.54623260e-01, 9.72781458e-01, 7.38364326e-01, 5.28075515e-01,\n",
      "        5.23957456e-01, 3.41672558e-01],\n",
      "       [3.15814865e-02, 6.07937192e-01, 6.53679262e-01, 3.27453818e-01,\n",
      "        6.76777394e-01, 4.62488664e-01, 9.01266500e-01, 6.56402229e-01,\n",
      "        1.15478851e-01, 3.94373869e-01],\n",
      "       [1.11830652e-01, 4.88954303e-01, 4.40703966e-01, 5.42151756e-01,\n",
      "        4.06791740e-01, 5.96882133e-02, 9.74115401e-02, 4.47540569e-02,\n",
      "        2.60298192e-01, 3.98298410e-01],\n",
      "       [2.43115898e-01, 2.22040820e-01, 9.00049287e-01, 3.63986226e-01,\n",
      "        4.92751773e-02, 5.90859878e-01, 6.54872154e-05, 7.49810917e-01,\n",
      "        1.32296892e-01, 1.12036079e-01],\n",
      "       [3.62119521e-01, 1.10666591e-01, 3.02133539e-01, 8.85039803e-01,\n",
      "        2.31804594e-01, 3.00205993e-01, 5.84297768e-01, 8.15377580e-01,\n",
      "        7.69070961e-01, 1.88108740e-01],\n",
      "       [2.47954015e-01, 4.00014202e-01, 1.82875693e-01, 1.06916816e-01,\n",
      "        8.43487772e-01, 4.10341911e-02, 6.72296829e-01, 6.35543788e-02,\n",
      "        5.59730263e-01, 1.21540565e-01],\n",
      "       [1.81694661e-01, 7.24313084e-01, 9.66243880e-01, 3.94107216e-01,\n",
      "        6.69694175e-01, 4.43933635e-01, 5.61014032e-01, 2.15449678e-01,\n",
      "        3.16888505e-01, 9.43661710e-01],\n",
      "       [7.26897362e-01, 3.37770379e-01, 5.69659940e-01, 6.86420602e-01,\n",
      "        4.14001694e-01, 9.51768669e-01, 9.10145095e-01, 7.46115166e-02,\n",
      "        5.68831973e-01, 2.30553105e-01],\n",
      "       [8.62730591e-01, 1.86902659e-01, 8.22006140e-01, 5.80240654e-01,\n",
      "        1.72571519e-01, 5.49427226e-01, 2.87475603e-01, 5.35622074e-01,\n",
      "        9.41543437e-01, 4.73551497e-01],\n",
      "       [3.36881379e-01, 1.51356104e-03, 7.85946307e-02, 2.58761157e-01,\n",
      "        2.36028717e-01, 4.22749362e-01, 4.65400623e-01, 1.46906071e-01,\n",
      "        3.54040360e-01, 6.87750932e-01],\n",
      "       [9.11124089e-01, 9.35535702e-01, 6.50903693e-01, 3.60959565e-01,\n",
      "        5.02542209e-01, 3.23817787e-01, 4.26370994e-01, 2.61088012e-01,\n",
      "        5.16706073e-01, 7.64992661e-01],\n",
      "       [1.78364256e-01, 5.65618246e-01, 6.35714169e-01, 6.77891786e-01,\n",
      "        8.03620744e-01, 3.52964900e-01, 3.29575595e-01, 9.02938126e-01,\n",
      "        6.91235630e-01, 3.25330476e-01],\n",
      "       [3.67138788e-01, 7.43556135e-01, 5.48680418e-01, 2.17252335e-01,\n",
      "        8.17147207e-01, 3.87637970e-01, 3.71010347e-01, 8.33626689e-01,\n",
      "        3.11320890e-01, 7.28070476e-01],\n",
      "       [2.73022645e-01, 1.77025933e-01, 2.29042537e-01, 8.40166642e-02,\n",
      "        7.15149803e-01, 2.54087329e-01, 8.74839841e-01, 5.27097859e-01,\n",
      "        2.07610093e-01, 3.07890699e-01],\n",
      "       [4.24628125e-01, 3.92948398e-01, 1.66715279e-01, 7.95387978e-01,\n",
      "        5.56544761e-01, 2.40916470e-01, 8.67509577e-02, 5.34065349e-01,\n",
      "        5.17429304e-01, 4.96080178e-02]])]\n",
      "\n",
      "Bias:  [array([[0.8131805 , 0.32090315, 0.09229554, 0.09523869, 0.78505156,\n",
      "        0.8327844 , 0.87539375, 0.29382316, 0.6475573 , 0.53225385,\n",
      "        0.87531318, 0.35518635, 0.61661557, 0.90694634, 0.66774526]]), array([[0.3480256 , 0.96584938, 0.94956374, 0.15607082, 0.62568527,\n",
      "        0.89873564, 0.21454596, 0.37487853, 0.48633038, 0.60468442]])]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Network():\n",
    "    def __init__(self, X, y, weights, biases, learning_rate, activation, activation_derivative):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.weights = weights\n",
    "        self.biases = biases\n",
    "        self.n_layers = len(weights)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.activation = activation\n",
    "        self.activation_derivative = activation_derivative\n",
    "\n",
    "    def softmax(self, z):\n",
    "        e_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return e_z / np.sum(e_z, axis=1, keepdims=True)\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_true):\n",
    "        return -np.sum(y_true * np.log(y_pred), axis=1)\n",
    "\n",
    "    def cross_entropy_derivative(self, y_pred, y_true):\n",
    "        return y_pred - y_true\n",
    "\n",
    "    def backpropagation(self):\n",
    "        z = [None] * self.n_layers\n",
    "        a = [None] * self.n_layers\n",
    "        a.insert(0, self.X)\n",
    "        for i in range(self.n_layers - 1):\n",
    "            z[i] = np.dot(a[i], self.weights[i]) + self.biases[i]\n",
    "            a[i + 1] = self.activation(z[i])\n",
    "        z[-1] = np.dot(a[-2], self.weights[-1]) + self.biases[-1]\n",
    "        a[-1] = self.softmax(z[-1])\n",
    "\n",
    "        # loss = np.mean(self.cross_entropy(a[-1], self.y)) #enable this if you want to see the loss\n",
    "\n",
    "        # Backward pass\n",
    "        deltas = [None] * self.n_layers\n",
    "        deltas[-1] = self.cross_entropy_derivative(a[-1], self.y)\n",
    "        for i in reversed(range(self.n_layers - 1)):\n",
    "            deltas[i] = np.dot(deltas[i + 1], self.weights[i + 1].T) * self.activation_derivative(z[i])\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            self.weights[i] -= self.learning_rate * np.dot(a[i].T, deltas[i]) / samples\n",
    "            self.biases[i] -= self.learning_rate * np.sum(deltas[i], axis=0, keepdims=True) / samples\n",
    "\n",
    "        return weights, biases\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "classes = 10 # number of neurons in output layer\n",
    "samples = 60000 # number of samples\n",
    "n_input = 784 # image dimension\n",
    "layers = [n_input, 20, 15, classes]  # Input layer, two hidden layers, output layer\n",
    "weights = [np.random.randn(layers[i], layers[i + 1]) for i in range(len(layers) - 1)]\n",
    "biases = [np.zeros((1, layers[i + 1])) for i in range(len(layers) - 1)]\n",
    "learning_rate = 0.01\n",
    "\n",
    "nn = Network(X, y, weights, biases, learning_rate, activation=sigmoid, activation_derivative=sigmoid_derivative)\n",
    "weights, biases = nn.backpropagation()\n",
    "print(\"\\nWeights: \", W)\n",
    "print(\"\\nBias: \", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6\n",
    "everything combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "Data loaded!\n",
      "Performing backpropagation...\n",
      "Backpropagation complete! Run the next cell to visualize.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Network():\n",
    "    def __init__(self, X, y, weights, biases, learning_rate, activation, activation_derivative, gamma, epochs):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.weights = weights\n",
    "        self.biases = biases\n",
    "        self.n_layers = len(weights)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.activation = activation\n",
    "        self.activation_derivative = activation_derivative\n",
    "        self.gamma = gamma\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def softmax(self, z):\n",
    "        e_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return e_z / np.sum(e_z, axis=1, keepdims=True)\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_true):\n",
    "        return -np.sum(y_true * np.log(y_pred), axis=1)\n",
    "\n",
    "    def cross_entropy_derivative(self, y_pred, y_true):\n",
    "        return y_pred - y_true\n",
    "\n",
    "    def backpropagation(self):\n",
    "        print(\"Performing backpropagation...\")\n",
    "\n",
    "        v_w = [np.zeros_like(w) for w in self.weights]\n",
    "        v_b = [np.zeros_like(b) for b in self.biases]\n",
    "\n",
    "        losses = np.array([])\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            print(f\"Epoch {epoch + 1}/{self.epochs}\", end='\\r')\n",
    "\n",
    "            # Forward pass\n",
    "            z = [None] * self.n_layers\n",
    "            a = [None] * self.n_layers\n",
    "            a.insert(0, self.X)\n",
    "            for i in range(self.n_layers - 1):\n",
    "                z[i] = np.dot(a[i], self.weights[i]) + self.biases[i]\n",
    "                a[i + 1] = self.activation(z[i])\n",
    "            z[-1] = np.dot(a[-2], self.weights[-1]) + self.biases[-1]\n",
    "            a[-1] = self.softmax(z[-1])\n",
    "\n",
    "            loss = self.cross_entropy(a[-1], self.y)\n",
    "            losses = np.append(losses, np.mean(loss))\n",
    "\n",
    "            # Backward pass\n",
    "            deltas = [None] * self.n_layers\n",
    "            deltas[-1] = self.cross_entropy_derivative(a[-1], self.y)\n",
    "            for i in reversed(range(self.n_layers - 1)):\n",
    "                deltas[i] = np.dot(deltas[i + 1], self.weights[i + 1].T) * self.activation_derivative(z[i])\n",
    "\n",
    "            for i in range(self.n_layers):\n",
    "                v_w[i] = self.gamma * v_w[i] + self.learning_rate * np.dot(a[i].T, deltas[i]) / samples\n",
    "                v_b[i] = self.gamma * v_b[i] + self.learning_rate * np.sum(deltas[i], axis=0, keepdims=True) / samples\n",
    "                self.weights[i] -= v_w[i]\n",
    "                self.biases[i] -= v_b[i]\n",
    "        \n",
    "        print(\"Backpropagation complete! Run the next cell to visualize.\")\n",
    "\n",
    "        return weights, biases, losses\n",
    "\n",
    "def data(file_path):\n",
    "    print(\"Reading data...\")\n",
    "    f = np.loadtxt(file_path, delimiter=',', skiprows=1)\n",
    "    y, X = np.hsplit(f, [1])\n",
    "    print(\"Data loaded!\")\n",
    "    return X/255, np.eye(classes)[y.astype(int).flatten()]\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "classes = 10 # number of neurons in output layer\n",
    "samples = 60000 # number of samples\n",
    "n_input = 784 # image dimension\n",
    "layers = [n_input, 20, 15, classes]  # Input layer, two hidden layers, output layer\n",
    "weights = [np.random.randn(layers[i], layers[i + 1]) for i in range(len(layers) - 1)]\n",
    "biases = [np.zeros((1, layers[i + 1])) for i in range(len(layers) - 1)]\n",
    "\n",
    "X, y = data('mnist_train.csv')\n",
    "nn = Network(X, y, weights, biases, learning_rate=0.01, activation=sigmoid, activation_derivative=sigmoid_derivative, gamma=0.9, epochs=100)\n",
    "weights, biases, losses = nn.backpropagation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "# enable this to see the weights, biases, and losses\n",
    "# print(\"\\nWeights: \", W)\n",
    "# print(\"\\nBias: \", b)\n",
    "# print(\"\\nLoss: \", losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
